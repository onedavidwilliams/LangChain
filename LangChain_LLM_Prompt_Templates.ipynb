{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbb21fe-1d76-45f9-9e58-efaa3fee905a",
   "metadata": {},
   "source": [
    "# Begginer's Journey into LangChain! Section 4 - Prompt Templates\n",
    "- Note: all of the credit goes to Andrei Dumitrescu from [ZeroToMastery.io](https://zerotomastery.io. All of the notes and a lot of the code is a mixture of his class and my own deviation for learning purposes, but I **urge** you to take a look at Zero-To-Mastery if you are looking to up your skills! Here we go.\n",
    "#### First: \n",
    "*(if you have not done so in the previous section(s) comlete this step by uncommenting below)* \\\n",
    "We will install the `requirements.txt`* located in the same directory as our notbook\\\n",
    "Next you can check the version of anything inside of your *`requirements.txt`*\n",
    "- utilize `pip show openai` (or anyother package in *`requirements.txt`*\n",
    "\n",
    "#### Second\n",
    "We will create a .env file within the same directory to hold our environmental veriables. Your should add this to your .gitignore as well\n",
    "The `.env` file will contain API keys. Such as: \n",
    "OPENAI_API_KEY=\"YOURKEYHERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f3df28-6efc-47c2-827e-d48f6b746560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0bdb3a4-5b30-4498-984e-38b7bf26dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f4d144-b4ad-45a3-b645-58cf372979ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdd0c24-aa04-4e33-a453-a28f2d314134",
   "metadata": {},
   "source": [
    "# Merging different parts of the course\n",
    "Here I will diverege significantly from the course for my own learning purposes.\n",
    "#### Templates + Schemas\n",
    "What I will be doing is taking a prompt template, populating the template with the correct information, then passing it along to a schema with a system message that gives the llm \"context\" of its purpose -- the schema will be *`messages[]`*. The LLM will then respond with the appropriate output -- which here, is draft an email. At this point we could just print the ouput, but what if we merged one more section and streamed the data in chunks?\n",
    "\n",
    "#### How it works\n",
    "We create *`llm`* from the *`ChatOpenAI`* class to stream the data later with *`llm.stream()`*.\\\n",
    "Then we create a prompt template with *`template`* and *`prompt_template`* to fromat the string with placeholders *({recpnt}, {tone}, and {topic})* -- note I have not tried using f-strings but I assume you can do the same; this is just an internal method. But if you stick with me  we will find out! \\\n",
    "Using *`prompt_template.fromat()`* we can take the placeholders *(ex: recpnt='John')* in *`template`* and populate *`prompt`* \n",
    "\n",
    "#### Finally\n",
    "We use a for loop to print the chunks from *`llm.stream(messages`* until completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a419eae6-08e9-4a4d-b30a-43b8fcd936f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey John,\n",
      "\n",
      "Just wanted to touch base about using AI in our software dev game plan. It's a game changer, buddy! I've seen how machine learning can drastically improve efficiency and accuracy in development. How about we explore investing in AI tools or even training our team in AI programming? It's a big leap, but it's worth taking, mate.\n",
      "\n",
      "Cheers,\n",
      "[Your Name]"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, AIMessage, HumanMessage\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4')\n",
    "template = '''Draft an email for {recpnt}. In this email use a {tone} tone and discuss {topic}. Please limit to 5 sentences.'''\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "\n",
    "prompt = prompt_template.format(recpnt='John', tone='informal', topic='AI development strategies for software dev agencies.')\n",
    "messages = [\n",
    "    SystemMessage('You are an experienced startup veteran and CEO with a background in programing.'),\n",
    "    HumanMessage(content=prompt)\n",
    "]\n",
    "\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e9deec-a005-4d62-aec8-192f1fc59060",
   "metadata": {},
   "source": [
    "#### If you wanted to be simpler or streaming is unecessary you can just print the output with *`llm.invoke(messages)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abb258-8f63-45ee-ace7-d204502d7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = llm.invoke(messages)\n",
    "# print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ddcab0-81af-41b7-932c-783d3a7b7924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
