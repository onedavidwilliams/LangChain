{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19757d5-be81-4f4c-8659-d55346ec6747",
   "metadata": {},
   "source": [
    "# Begginer's Journey into LangChain! Section 7 - Simple Sequential Chains\n",
    "- Note: all of the credit goes to Andrei Dumitrescu from [ZeroToMastery.io](https://zerotomastery.io. All of the notes and a lot of the code is a mixture of his class and my own deviation for learning purposes, but I **urge** you to take a look at Zero-To-Mastery if you are looking to up your skills! Here we go.\n",
    "#### First: \n",
    "*(if you have not done so in the previous section(s) comlete this step by uncommenting below)* \\\n",
    "We will install the `requirements.txt`* located in the same directory as our notbook\\\n",
    "Next you can check the version of anything inside of your *`requirements.txt`*\n",
    "- utilize `pip show openai` (or anyother package in *`requirements.txt`*\n",
    "\n",
    "#### Second\n",
    "We will create a .env file within the same directory to hold our environmental veriables. Your should add this to your .gitignore as well\n",
    "The `.env` file will contain API keys. Such as: \n",
    "OPENAI_API_KEY=\"YOURKEYHERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "230a6d50-adf9-4ba6-9345-de6dac460468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75c67dca-12ec-4868-802b-cd3b294a79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04db5311-70b9-4634-a964-465c908d901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307d9cbf-f37b-4187-ad31-bd63bc7b14c4",
   "metadata": {},
   "source": [
    "# Using Simple Sequential chains\n",
    "In a SimpleSequentialChain the output of one chain is entered into a template of another prompt and passed backed to OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d24b605-07e0-433d-aff6-f0bc5f70400b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mA for loop in Python that removes duplicates from a list is not efficient due to several reasons:\n",
      "\n",
      "1. **Time Complexity**: The time complexity of a for loop in Python that removes duplicates from a list is O(n^2). This is because for each element in the list, the program must scan the entire list to check for duplicates. As the list grows, the time it takes to process the list increases exponentially.\n",
      "\n",
      "2. **Space Complexity**: When removing duplicates, you might create a new list to store the unique elements. This means you're using additional memory proportional to the size of the original list, leading to O(n) space complexity.\n",
      "\n",
      "3. **Mutability**: Lists in Python are mutable, meaning they can be changed. When you remove an element from a list, all the elements that come after it have to be shifted down. This can be costly in terms of performance, especially for large lists.\n",
      "\n",
      "4. **Stability**: If you want to maintain the original order of elements, using a for loop to remove duplicates can be tricky and error-prone. You must ensure that you only remove subsequent duplicates and not the first occurrence.\n",
      "\n",
      "An efficient way to remove duplicates from a list in Python is by using a data structure known as a set. A set is an unordered collection of unique elements. Converting a list to a set automatically removes any duplicate elements because sets only allow unique elements. However, this method does not preserve the original order of elements.\n",
      "\n",
      "If you need to preserve the order of elements, you can use the `dict.fromkeys()` function which removes duplicates while maintaining the order. This is efficient and faster as it internally uses a hash map for lookups, providing O(1) time complexity. However, this method is only available in Python 3.7 and later versions. \n",
      "\n",
      "Here is an example of using `dict.fromkeys()`:\n",
      "\n",
      "```python\n",
      "def remove_duplicates(input_list):\n",
      "    return list(dict.fromkeys(input_list))\n",
      "```\n",
      "\n",
      "And here is an example of using set:\n",
      "\n",
      "```python\n",
      "def remove_duplicates(input_list):\n",
      "    return list(set(input_list))\n",
      "```\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mYour last examples show how to efficiently remove duplicates from a list using the `dict.fromkeys()` function and the Python set. The `dict.fromkeys()` version is especially useful, as it retains the original order of elements while removing duplicates.\n",
      "\n",
      "However, let me clarify your questions by delving deeper into why a manual `for` loop isn't as efficient with step by step example:\n",
      "\n",
      "Let's consider this list with some duplicates:\n",
      "\n",
      "```python\n",
      "genres = [\"rock\", \"pop\", \"rock\", \"metal\", \"pop\", \"jazz\", \"jazz\"]\n",
      "```\n",
      "\n",
      "If we were to remove duplicates manually using a `for` loop, our code may look like this:\n",
      "\n",
      "```python\n",
      "new_list = []\n",
      "for i in genres:\n",
      "    if i not in new_list:\n",
      "        new_list.append(i)\n",
      "print(new_list)\n",
      "```\n",
      "\n",
      "Now, this script works and prints ['rock', 'pop', 'metal', 'jazz'] as expected. But looking specifically at the line that checks `if i not in new_list:`, this is where the inefficiency lies. \n",
      "\n",
      "For *each* item we are looping over in genres, Python has to compare it against *every* item already inside new_list to check if it's a duplicate or not. This gives rise to O(n^2) complexity because for n items in our list, we're potentially making n comparisons to verify duplicates. \n",
      "\n",
      "Your explanation of creating a new list and this operation taking extra space, reflecting a O(n) space complexity, is correct. Similarly, the aspect of mutability also adds to inefficiency due to shift in index when deleting an item.\n",
      "\n",
      "Lastly, your point about stability is rightâ€“using a `for` loop to make sure you're only deleting subsequent duplicates to maintain original order can be quite error-prone. A simpler, more pythonic, and efficient solution is using set or `dict.fromkeys()` as described in the second part of your question. \n",
      "\n",
      "However, do note though that earlier versions of Python before version 3.7 do not maintain the order of element insertion in their dictionary objects, so difference in Python versions is also a point to concern.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "code_llm = ChatOpenAI(model_name='gpt-4', temperature=0.5)\n",
    "code_prompt = PromptTemplate.from_template(\n",
    "    template='You are an experienced programer that writes a function that implements:\\n {concept}'\n",
    ")\n",
    "chain1 = LLMChain(llm=code_llm, prompt=code_prompt)\n",
    "\n",
    "descript_llm = ChatOpenAI(model_name='gpt-4', temperature=1.2)\n",
    "descript_prompt = PromptTemplate.from_template(\n",
    "    template='Please ecplain exactly how {function} works.'\n",
    ")\n",
    "chain2 = LLMChain(llm=descript_llm,prompt=descript_prompt)\n",
    "\n",
    "main_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "output = main_chain.invoke('The details of why a for loop in python that removes duplicates from a list is not effecient.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8256deb3-8785-4e7f-848f-7cbf5c9b7284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'The details of why a for loop in python that removes duplicates from a list is nt effecient.', 'output': 'Your last examples show how to efficiently remove duplicates from a list using the `dict.fromkeys()` function and the Python set. The `dict.fromkeys()` version is especially useful, as it retains the original order of elements while removing duplicates.\\n\\nHowever, let me clarify your questions by delving deeper into why a manual `for` loop isn\\'t as efficient with step by step example:\\n\\nLet\\'s consider this list with some duplicates:\\n\\n```python\\ngenres = [\"rock\", \"pop\", \"rock\", \"metal\", \"pop\", \"jazz\", \"jazz\"]\\n```\\n\\nIf we were to remove duplicates manually using a `for` loop, our code may look like this:\\n\\n```python\\nnew_list = []\\nfor i in genres:\\n    if i not in new_list:\\n        new_list.append(i)\\nprint(new_list)\\n```\\n\\nNow, this script works and prints [\\'rock\\', \\'pop\\', \\'metal\\', \\'jazz\\'] as expected. But looking specifically at the line that checks `if i not in new_list:`, this is where the inefficiency lies. \\n\\nFor *each* item we are looping over in genres, Python has to compare it against *every* item already inside new_list to check if it\\'s a duplicate or not. This gives rise to O(n^2) complexity because for n items in our list, we\\'re potentially making n comparisons to verify duplicates. \\n\\nYour explanation of creating a new list and this operation taking extra space, reflecting a O(n) space complexity, is correct. Similarly, the aspect of mutability also adds to inefficiency due to shift in index when deleting an item.\\n\\nLastly, your point about stability is rightâ€“using a `for` loop to make sure you\\'re only deleting subsequent duplicates to maintain original order can be quite error-prone. A simpler, more pythonic, and efficient solution is using set or `dict.fromkeys()` as described in the second part of your question. \\n\\nHowever, do note though that earlier versions of Python before version 3.7 do not maintain the order of element insertion in their dictionary objects, so difference in Python versions is also a point to concern.'}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dde2668-36eb-4053-bbf1-64918ca63297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
